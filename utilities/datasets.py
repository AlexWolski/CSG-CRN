import os
import math
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import Dataset
from utilities.constants import TARGET_SAMPLING, UNIFIED_SAMPLING, NEAR_SURFACE_SAMPLE_FACTOR
from utilities.data_augmentation import augment_sample_batch, augment_sample_batch_points
from utilities.data_processing import UNIFORM_FOLDER, SURFACE_FOLDER, NEAR_SURFACE_FOLDER


class PointDataset(Dataset):
	def __init__(self, file_rel_paths, device, args, augment_data=False, loss_sampling_method=UNIFIED_SAMPLING, input_sampling_method=TARGET_SAMPLING, dataset_name="Dataset"):
		self.file_rel_paths = file_rel_paths
		self.augmented_copies = len(file_rel_paths) * args.augment_copies
		self.raw_copies = len(file_rel_paths)
		self.device = device
		self.args = args
		self.augment_data = augment_data
		self.dataset_name = dataset_name
		self.loss_sampling_method = loss_sampling_method
		self.input_sampling_method = input_sampling_method

		# Compute number of uniform and near-surface SDF samples to load
		self.num_uniform_input_samples = math.ceil(self.args.num_input_points * self.args.surface_uniform_ratio)
		self.num_near_surface_input_samples = self.args.num_input_points - self.num_uniform_input_samples

		# Compute the number of surface and uniform loss samples based on the surface-uniform ratio.
		self.num_uniform_loss_samples = math.ceil(self.args.num_loss_points * self.args.surface_uniform_ratio)
		self.num_near_surface_loss_samples = self.args.num_loss_points - self.num_uniform_loss_samples

		# Validate that the dataset contains enough samples for the configuration.
		self.__validate_dataset_size()
		self.__calculate_num_samples()
		self.__load_data_set()


	# Calculate the number of samples to load from the dataset based on the sampling methods
	def __calculate_num_samples(self):
		self.num_uniform_samples = self.num_uniform_input_samples + self.num_uniform_loss_samples
		self.num_near_surface_samples = self.num_near_surface_input_samples + self.num_near_surface_loss_samples

		# Find the number of near-surface samples that will be generated from unified samples.
		num_unified_input_samples = self.num_near_surface_input_samples if self.input_sampling_method == UNIFIED_SAMPLING else None
		num_unified_loss_samples = self.num_near_surface_loss_samples if self.loss_sampling_method == UNIFIED_SAMPLING else None

		# Near-surface samples will be generated by sampling a largeer number of uniform samples based on their distance to both the target and reconstruction SDFs.
		num_total_unified_samples = (num_unified_input_samples or 0) + (num_unified_loss_samples or 0)
		num_uniform_samples_for_unified = num_total_unified_samples * NEAR_SURFACE_SAMPLE_FACTOR
		max_unified_samples = self.args.dataset_num_sdf_samples - self.num_uniform_samples

		# Adjust the number of uniform samples loaded if the provided dataset does not contain enough.
		if num_uniform_samples_for_unified > max_unified_samples:
			print('WARNING: Insufficient sample points in dataset.')
			print(f'With Unified Sampling enabled, {num_uniform_samples_for_unified} uniform samples are recommended to generate the target {self.num_near_surface_samples} unified near-surface samples.')
			print(f'The provided dataset is only large enough to allocate {max_unified_samples} samples for unified near-surface sampling.\n')

			# Divide the number of available uniform samples between input and loss near-surface samples
			if num_unified_input_samples and num_unified_loss_samples:
				input_ratio = num_unified_input_samples / num_total_unified_samples
				loss_ratio = num_unified_loss_samples / num_total_unified_samples
				self.num_near_surface_input_samples = math.floor(max_unified_samples * input_ratio)
				self.num_near_surface_loss_samples = math.floor(max_unified_samples * loss_ratio)
			elif num_unified_input_samples:
				self.num_near_surface_input_samples = max_unified_samples
			elif num_unified_loss_samples:
				self.num_near_surface_loss_samples = max_unified_samples
		# Dataset contain sufficient samples for generating all near-surface samples
		else:
			if num_unified_input_samples:
				self.num_near_surface_input_samples = num_unified_input_samples * NEAR_SURFACE_SAMPLE_FACTOR
			if num_unified_loss_samples:
				self.num_near_surface_loss_samples = num_unified_loss_samples * NEAR_SURFACE_SAMPLE_FACTOR

		# Adjust `num_uniform_samples` and `num_near_surface_samples` as they represent the number of samples to load from the dataset.
		self.num_uniform_samples += num_uniform_samples_for_unified
		self.num_near_surface_samples -= num_total_unified_samples


	def __validate_dataset_size(self):
		# Ensure that the dataset is sufficiently large for both the near-surface and uniformsamples
		# Note that the dataset should contain an equal amount of near-surface and uniform samples.
		total_near_surface_samples = self.num_near_surface_input_samples + self.num_near_surface_loss_samples
		total_uniform_loss_samples = self.num_uniform_input_samples + self.num_uniform_loss_samples
		excpetion_message = ""

		if total_near_surface_samples > self.args.dataset_num_sdf_samples:
			excpetion_message += f'Insufficient number of near-surface samples. Configuration set to {total_near_surface_samples} samples but the dataset only contains {args.dataset_num_sdf_samples} samples.\n'

		if total_uniform_loss_samples > self.args.dataset_num_sdf_samples:
			excpetion_message += f'Insufficient number of uniform samples. Configuration set to {total_uniform_loss_samples} samples but the dataset only contains {args.dataset_num_sdf_samples} samples.\n'

		if excpetion_message != "":
			raise Exception(excpetion_message)


	# Load all samples into system memory
	def __load_data_set(self):
		data_sample_list = []

		for file_rel_path in tqdm(self.file_rel_paths, desc=f'Loading {self.dataset_name}'):
			data_sample_list.append(self.__load_data_sample(file_rel_path))

		# Omit skipped samples
		skipped_samples = data_sample_list.count(None)
		self.raw_copies = len(self.file_rel_paths) - skipped_samples

		if skipped_samples > 0:
			data_sample_list = [i for i in data_sample_list if i != None]

		# Save sample lists as tensors
		uniform_sample_list, near_surface_sample_list, surface_sample_list = list(zip(*data_sample_list))
		self.uniform_samples = torch.stack(uniform_sample_list, dim=0)
		self.near_surface_samples = torch.stack(near_surface_sample_list, dim=0)
		self.surface_samples = torch.stack(surface_sample_list, dim=0)


	def __load_data_sample(self, file_rel_path):
		try:
			# Load SDF samples
			(uniform_samples, near_surface_samples) = self.__load_sdf_samples(file_rel_path)
			# Load surface samples
			surface_samples = self.__load_surface_samples(file_rel_path)

			return (uniform_samples, near_surface_samples, surface_samples)

		# Skip samples that fail to load
		except Exception as e:
			print(e)
			return None


	def __load_sdf_samples(self, file_rel_path):
		# Load uniform and near-surface SDF samples from file
		uniform_samples = self.__load_point_samples(UNIFORM_FOLDER, file_rel_path, self.num_uniform_samples)
		near_surface_samples = self.__load_point_samples(NEAR_SURFACE_FOLDER, file_rel_path, self.num_near_surface_samples)

		return (uniform_samples, near_surface_samples)


	def __load_surface_samples(self, file_rel_path):
		return self.__load_point_samples(SURFACE_FOLDER, file_rel_path, self.args.num_val_acc_points)


	# Load a specified number of point samples from a numpy file
	def __load_point_samples(self, subfolder, file_rel_path, num_point_samples=None):
		# Open numpy file as memmap
		file_path = os.path.join(self.args.data_dir, subfolder, file_rel_path)

		if not os.path.isfile(file_path):
			print('FILE')
			FileNotFoundError(f'Unable to find data sample file:\n{file_path}')

		samples_mmap = np.load(file_path, mmap_mode='r')
		file_length = samples_mmap.shape[0]

		if num_point_samples == None:
			num_point_samples = file_length
		elif num_point_samples > file_length:
			print('LENGTH')
			raise Exception(f'Failed to read {num_point_samples} samples from file with {file_length} samples:\n{file_path}')

		# Copy the required number of samples into memory and convert to a torch tensor
		samples = samples_mmap[:num_point_samples].copy().astype(np.float32)
		return torch.from_numpy(samples)


	def __augment_sdf_samples(self, sdf_samples):
		sdf_points = sdf_samples[:,:,:3]
		sdf_distances = sdf_samples[:,:,3]
		sdf_distances = sdf_distances.unsqueeze(-1)

		augmented_points, augmented_distances = augment_sample_batch(sdf_points, sdf_distances, self.args)
		return torch.cat((augmented_points, augmented_distances), dim=-1)


	def __len__(self):
		return self.augmented_copies


	def __getitem__(self, batch_idx):
		# Adjust indices for augmented copies
		if self.augmented_copies > self.raw_copies:
			batch_idx = [index % self.raw_copies for index in batch_idx]

		# Load batch samples and send to target device
		batch_uniform_samples = self.uniform_samples[batch_idx].to(self.device)
		batch_near_surface_samples = self.near_surface_samples[batch_idx].to(self.device)
		batch_surface_samples = self.surface_samples[batch_idx].to(self.device) if self.surface_samples != None else None

		# Augment samples
		if self.augment_data:
			combined_samples = torch.cat((batch_uniform_samples, batch_near_surface_samples), dim=1)
			combined_samples = self.__augment_sdf_samples(combined_samples)
			batch_uniform_samples = combined_samples[:, :self.num_uniform_samples]
			batch_near_surface_samples = combined_samples[:, self.num_uniform_samples:]

			if batch_surface_samples != None:
				batch_surface_samples = augment_sample_batch_points(batch_surface_samples, self.args)

		uniform_tensor_sizes = [self.num_uniform_input_samples, self.num_uniform_loss_samples]
		near_surface_tensor_sizes = [self.num_near_surface_input_samples, self.num_near_surface_loss_samples]
		all_tensor_sizes = uniform_tensor_sizes + near_surface_tensor_sizes

		# When unified input and loss sampling is enabled, all tensors are populated with uniform samples.
		if self.input_sampling_method == UNIFIED_SAMPLING and self.loss_sampling_method == UNIFIED_SAMPLING:
			(
				batch_uniform_input_samples,
				batch_uniform_loss_samples,
				batch_near_surface_input_samples,
				batch_near_surface_loss_samples
			) = torch.split(batch_uniform_samples, all_tensor_sizes, dim=1)
		# Unified input sampling
		elif self.input_sampling_method == UNIFIED_SAMPLING:
			uniform_tensor_sizes += [self.num_near_surface_input_samples]
			(batch_uniform_input_samples, batch_uniform_loss_samples, batch_near_surface_input_samples) = torch.split(batch_uniform_samples, uniform_tensor_sizes, dim=1)
			batch_near_surface_loss_samples = batch_near_surface_samples[:, :self.num_near_surface_loss_samples]
		# Unified loss sampling
		elif self.loss_sampling_method == UNIFIED_SAMPLING:
			uniform_tensor_sizes += [self.num_near_surface_loss_samples]
			(batch_uniform_input_samples, batch_uniform_loss_samples, batch_near_surface_loss_samples) = torch.split(batch_uniform_samples, uniform_tensor_sizes, dim=1)
			batch_near_surface_input_samples = batch_near_surface_samples[:, :self.num_near_surface_input_samples]
		# When using target sampling, separate input and loss samples using the standard number of samples.
		else:
			(batch_uniform_input_samples, batch_uniform_loss_samples) = torch.split(batch_uniform_samples, uniform_tensor_sizes, dim=1)
			(batch_near_surface_input_samples, batch_near_surface_loss_samples) = torch.split(batch_near_surface_samples, near_surface_tensor_sizes, dim=1)

		data_sample = (
			batch_uniform_input_samples.detach(),
			batch_uniform_loss_samples.detach(),
			batch_near_surface_input_samples.detach(),
			batch_near_surface_loss_samples.detach(),
			batch_surface_samples.detach()
		)

		return data_sample
