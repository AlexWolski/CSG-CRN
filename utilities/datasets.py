import os
import math
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import Dataset
from utilities.constants import UNIFIED_SAMPLING, NEAR_SURFACE_SAMPLE_FACTOR
from utilities.data_augmentation import augment_sample_batch, augment_sample_batch_points
from utilities.data_processing import UNIFORM_FOLDER, SURFACE_FOLDER, NEAR_SURFACE_FOLDER


class PointDataset(Dataset):
	def __init__(self, file_rel_paths, device, args, augment_data=False, sampling_method=UNIFIED_SAMPLING, dataset_name="Dataset"):
		self.file_rel_paths = file_rel_paths
		self.augmented_copies = len(file_rel_paths) * args.augment_copies
		self.raw_copies = len(file_rel_paths)
		self.device = device
		self.args = args
		self.augment_data = augment_data
		self.dataset_name = dataset_name
		self.sampling_method = sampling_method

		# Compute number of uniform and near-surface SDF samples to load
		self.num_uniform_input_samples = math.ceil(self.args.num_input_points * self.args.surface_uniform_ratio)
		self.num_near_surface_input_samples = self.args.num_input_points - self.num_uniform_input_samples

		# Compute the number of surface and uniform loss samples based on the surface-uniform ratio.
		self.num_uniform_loss_samples = math.ceil(self.args.num_loss_points * self.args.surface_uniform_ratio)
		self.num_near_surface_loss_samples = self.args.num_loss_points - self.num_uniform_loss_samples

		# Validate that the dataset contains enough samples for the configuration.
		self.__validate_dataset_size()

		self.num_uniform_samples = self.num_uniform_input_samples + self.num_uniform_loss_samples
		self.num_near_surface_samples = self.num_near_surface_input_samples + self.num_near_surface_loss_samples

		# When the loss is computed on both target and reconstruction near-surface samples,
		# increase the number of uniform points loaded to allow proximity selection. No near-surface samples need to be loaded.
		if self.sampling_method == UNIFIED_SAMPLING:
			# Near-surface samples will generated by sampling a largeer number of uniform samples based on their distance to both the target and reconstruction SDFs.
			num_proximity_loss_samples = self.num_near_surface_loss_samples * NEAR_SURFACE_SAMPLE_FACTOR

			# Adjust the number of uniform samples loaded if the provided dataset does not contain enough.
			max_proximity_uniform_loss_samples = args.dataset_num_sdf_samples - self.num_uniform_samples

			if self.num_near_surface_loss_samples > max_proximity_uniform_loss_samples:
				print('WARNING: Insufficient sample points in dataset.')
				print(f'With Unified Sampling enabled, {self.num_near_surface_loss_samples} uniform samples are recommended to generate the target {num_proximity_loss_samples} unified near-surface loss samples.')
				print(f'The provided dataset is only large enough to allocate {max_proximity_uniform_loss_samples} samples for unified near-surface sampling.\n')

				self.num_near_surface_loss_samples = max_proximity_uniform_loss_samples
			else:
				self.num_near_surface_loss_samples = num_proximity_loss_samples

			# Adjust `num_uniform_samples` and `num_near_surface_samples` as they represent the number of samples to load from the dataset.
			self.num_uniform_samples += self.num_near_surface_loss_samples
			self.num_near_surface_samples = self.num_near_surface_input_samples

		self.__load_data_set()


	def __validate_dataset_size(self):
		# Ensure that the dataset is sufficiently large for both the near-surface and uniformsamples
		# Note that the dataset should contain an equal amount of near-surface and uniform samples.
		total_near_surface_samples = self.num_near_surface_input_samples + self.num_near_surface_loss_samples
		total_uniform_loss_samples = self.num_uniform_input_samples + self.num_uniform_loss_samples
		excpetion_message = ""

		if total_near_surface_samples > self.args.dataset_num_sdf_samples:
			excpetion_message += f'Insufficient number of near-surface samples. Configuration set to {total_near_surface_samples} samples but the dataset only contains {args.dataset_num_sdf_samples} samples.\n'

		if total_uniform_loss_samples > self.args.dataset_num_sdf_samples:
			excpetion_message += f'Insufficient number of uniform samples. Configuration set to {total_uniform_loss_samples} samples but the dataset only contains {args.dataset_num_sdf_samples} samples.\n'

		if excpetion_message != "":
			raise Exception(excpetion_message)


	# Load all samples into system memory
	def __load_data_set(self):
		data_sample_list = []

		for file_rel_path in tqdm(self.file_rel_paths, desc=f'Loading {self.dataset_name}'):
			data_sample_list.append(self.__load_data_sample(file_rel_path))

		# Omit skipped samples
		skipped_samples = data_sample_list.count(None)
		self.raw_copies = len(self.file_rel_paths) - skipped_samples

		if skipped_samples > 0:
			data_sample_list = [i for i in data_sample_list if i != None]

		# Save sample lists as tensors
		uniform_sample_list, near_surface_sample_list, surface_sample_list = list(zip(*data_sample_list))
		self.uniform_samples = torch.stack(uniform_sample_list, dim=0)
		self.near_surface_samples = torch.stack(near_surface_sample_list, dim=0)
		self.surface_samples = torch.stack(surface_sample_list, dim=0)


	def __load_data_sample(self, file_rel_path):
		try:
			# Load SDF samples
			(uniform_samples, near_surface_samples) = self.__load_sdf_samples(file_rel_path)
			# Load surface samples
			surface_samples = self.__load_surface_samples(file_rel_path)

			return (uniform_samples, near_surface_samples, surface_samples)

		# Skip samples that fail to load
		except Exception as e:
			print(e)
			return None


	def __load_sdf_samples(self, file_rel_path):
		# Load uniform and near-surface SDF samples from file
		uniform_samples = self.__load_point_samples(UNIFORM_FOLDER, file_rel_path, self.num_uniform_samples)
		near_surface_samples = self.__load_point_samples(NEAR_SURFACE_FOLDER, file_rel_path, self.num_near_surface_samples)

		return (uniform_samples, near_surface_samples)


	def __load_surface_samples(self, file_rel_path):
		return self.__load_point_samples(SURFACE_FOLDER, file_rel_path, self.args.num_val_acc_points)


	# Load a specified number of point samples from a numpy file
	def __load_point_samples(self, subfolder, file_rel_path, num_point_samples=None):
		# Open numpy file as memmap
		file_path = os.path.join(self.args.data_dir, subfolder, file_rel_path)

		if not os.path.isfile(file_path):
			print('FILE')
			FileNotFoundError(f'Unable to find data sample file:\n{file_path}')

		samples_mmap = np.load(file_path, mmap_mode='r')
		file_length = samples_mmap.shape[0]

		if num_point_samples == None:
			num_point_samples = file_length
		elif num_point_samples > file_length:
			print('LENGTH')
			raise Exception(f'Failed to read {num_point_samples} samples from file with {file_length} samples:\n{file_path}')

		# Copy the required number of samples into memory and convert to a torch tensor
		samples = samples_mmap[:num_point_samples].copy().astype(np.float32)
		return torch.from_numpy(samples)


	def __augment_sdf_samples(self, sdf_samples):
		sdf_points = sdf_samples[:,:,:3]
		sdf_distances = sdf_samples[:,:,3]
		sdf_distances = sdf_distances.unsqueeze(-1)

		augmented_points, augmented_distances = augment_sample_batch(sdf_points, sdf_distances, self.args)
		return torch.cat((augmented_points, augmented_distances), dim=-1)


	def __len__(self):
		return self.augmented_copies


	def __getitem__(self, batch_idx):
		# Adjust indices for augmented copies
		if self.augmented_copies > self.raw_copies:
			batch_idx = [index % self.raw_copies for index in batch_idx]

		# Load batch samples and send to target device
		batch_uniform_samples = self.uniform_samples[batch_idx].to(self.device)
		batch_near_surface_samples = self.near_surface_samples[batch_idx].to(self.device)
		batch_surface_samples = self.surface_samples[batch_idx].to(self.device) if self.surface_samples != None else None

		# Augment samples
		if self.augment_data:
			combined_samples = torch.cat((batch_uniform_samples, batch_near_surface_samples), dim=1)
			combined_samples = self.__augment_sdf_samples(combined_samples)
			batch_uniform_samples = combined_samples[:, :self.num_uniform_samples]
			batch_near_surface_samples = combined_samples[:, self.num_uniform_samples:]

			if batch_surface_samples != None:
				batch_surface_samples = augment_sample_batch_points(batch_surface_samples, self.args)

		# Separate input and loss samples
		batch_uniform_input_samples = batch_uniform_samples[:, :self.num_uniform_input_samples]
		batch_near_surface_input_samples = batch_near_surface_samples[:, :self.num_near_surface_input_samples]

		# When unified sampling is enabled, the near-surface loss tensor is populated with uniform samples.
		if self.sampling_method == UNIFIED_SAMPLING:
			loss_samples = batch_uniform_samples[:, self.num_uniform_input_samples:]
			batch_uniform_loss_samples = loss_samples[:, self.num_uniform_loss_samples:]
			batch_near_surface_loss_samples = loss_samples[:, :self.num_near_surface_loss_samples]
		else:
			batch_uniform_loss_samples = batch_uniform_samples[:, self.num_uniform_input_samples:]
			batch_near_surface_loss_samples = batch_near_surface_samples[:, self.num_near_surface_input_samples:]

		data_sample = (
			batch_uniform_input_samples.detach(),
			batch_uniform_loss_samples.detach(),
			batch_near_surface_input_samples.detach(),
			batch_near_surface_loss_samples.detach(),
			batch_surface_samples.detach()
		)

		return data_sample
